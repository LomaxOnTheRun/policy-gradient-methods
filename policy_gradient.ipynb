{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "policy_gradient.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPNIP3u5RuO7qRtStFz4D3W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LomaxOnTheRun/policy-gradient-methods/blob/master/policy_gradient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqQt_wXn17cZ",
        "colab_type": "text"
      },
      "source": [
        "# Policy Gradient\n",
        "\n",
        "This is going to be an overview of a basic policy gradient method, with a view to expand to actor-critic and PPO methods in later notebooks.\n",
        "\n",
        "## Environment\n",
        "\n",
        "To test our algorithm I'm going to use a basic gridworld with a fixed layout and starting state (`s0`), a positive terminal state (`+1`), a negative terminal state (`-1`). The outer border cannot be crossed and the filled position (`###`) cannot be entered; an attempt to do either will simply leave the agent where it began.\n",
        "\n",
        "A visual representation of this gridworld is given below:\n",
        "\n",
        "```\n",
        "+------+------+------+------+\n",
        "|      |      |      |  +1  |\n",
        "+------+------+------+------+\n",
        "|      |######|      |  -1  |\n",
        "+------+------+------+------+\n",
        "|  s0  |      |      |      |\n",
        "+------+------+------+------+\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-C3onta-1MXd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class State:\n",
        "    def __init__(x, y, reward):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.reward = reward\n",
        "\n",
        "    def is_terminal(self):\n",
        "        return self.reward != 0\n",
        "\n",
        "def create_gridworld():\n",
        "    \"\"\"Return a dict of (x, y) coords to State object.\"\"\"\n",
        "    gridword = {}\n",
        "    for x in range(4):\n",
        "        for y in range(3):\n",
        "            # Simply ignore the block\n",
        "            if x == 1 and y == 1:\n",
        "                continue\n",
        "            reward = 0\n",
        "            if x == 3 and y == 0: reward = 1\n",
        "            if x == 3 and y == 1: reward = -1\n",
        "            gridworld[(x, y)] = State(x, y, reward)\n",
        "    return gridworld\n",
        "\n",
        "def get_new_state(gridword, state, direction):\n",
        "    \"\"\"Return the new state, after trying to move in a direction, (dx, dy).\"\"\"\n",
        "    new_x = state.x + direction[0]\n",
        "    new_y = state.y + direction[1]\n",
        "    new_state = gridworld.get((new_x, new_y), state)\n",
        "    return new_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyWSUxa13Tcc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKnG0fOa-D6g",
        "colab_type": "text"
      },
      "source": [
        "## Policy gradient\n",
        "\n",
        "Policy gradient methods attempt to learn a policy without needing to learn either an underlying model of the environment or even the values states, which could then be used to construct a policy (e.g. by moving toward the next reachable state with the highest value).\n",
        "\n",
        "A policy is dependent on the current state, $s$, and can be described by a set of parameters, $\\theta$. In a simple case these parameters can each represent a single state, but in more complex cases could be the weights of a neural network.\n",
        "\n",
        "From [Sutton and Barto](http://incompleteideas.net/book/RLbook2020.pdf) we get the "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4E2lOz3VWCwg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ad430350-caaf-43b4-fb2b-31c5f7598657"
      },
      "source": [
        "import gym\n",
        "\n",
        "env = gym.make('FrozenLake8x8-v0')\n",
        "\n",
        "print(env.nS, env.nA)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64 4\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}